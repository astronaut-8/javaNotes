# JUC

## 什么是线程安全

线程安全问题(在临界区(**多线程读写变量的位置**)发生竞态条件(由于多线程导致的结果无法预测或者说可能不符合我们的预期))

线程安全问题侧重对共享资源访问的数据不一致问题

线程安全 - 代码运行完全会复合预期，或者说代码在单线程运行下和多线程结果应该是一致的



线程安全的基本特征是原子性，有序性，可见性，特征不满足了，也是一种线程安全问题





## 常见并发问题

并发问题指的是因为线程调度，执行顺序，资源竞争造成的线程执行问题，不仅仅关注数据一致性

可以说其包含线程安全问题，更多包含一下侧重于线程的问题



并发问题产生的核心还是多线程并发环境下对共享资源的竞争，只不过关注点在于这个竞争对线程的影响，而不偏向于共享资源

- 很多线程往往要去对同一个资源作竞争，设计资源协调的工作

- 线程由操作系统调度，调度往往是随机的或者方式固定的(时间片轮询)代码的执行存在偶然顺序

- 线程需要彼此协作通信，这一步没做好也会有问题

- 原子性，可见性，有序性

  

  上述为程序系统的角度，从发展和技术的角度来看

- CPU从单核变为多核，并发编程是一个必然

- 互联网环境越来越复杂，需要并发执行的接口越来越多，并发编程是一个必然

- 异步编程技术的发展，比如NIO，需要协调并发

- 共享变量这一事物是必然存在的，我们一定会去并发读写共享变量的(锁对象也是对象)



- 死锁
- 活锁 - 多个线程由于需要不断响应对方的操作而不断改变自己的状态，导致程序无法推进，与死锁相反，他们没有被阻塞，但是进入无效忙碌的循环（双方都在尝试积极解决问题，但是协调不当进入无限循环）
- 线程🐔饿



一般的线程安全问题，加锁，volatile

规范线程通信

解决死锁，超时断开，顺序加锁

降低锁力度，减少资源冲突

## 对JMM的理解

- 首先Cpu快速发展，缓存模型持续变跟，cpu对代码的优化持续进步导致我们的代码出现可见性，有序性，原子性问题
- 我们需要通过很多手段来解决线程安全问题，比如从系统层面使用mesi协议，指令方面插入读写屏障，这些东西可以解决问题
- 但是我们无法兼容或者不断适应所有的cpu模型，所以我们抽象出JMM内存模型(整体分为主存和线程内存缓存)
- JMM模型和cpu模型进行兼容，使得我们的程序仅仅面向JMM模型，屏蔽不同硬件和操作系统的差异
- 我们通过关键字synchronized，volatile来规范原子可见有序问题，使得我们可以以统一的方式控制存储与代码逻辑，

## 多线程优势

并行- 发挥多核cpu优势

并发 - 提高效率

## 保证线程执行顺序

- 使用一个volatile的共享变量调控，每个线程运行前判断共享变量的值是否符合预期，否则sleep一段时间，一个线程运行完，对共享变量做修改
- join函数，使用一个main方法控制(t1.run t1.join t2.run t2.join ...) 或者提前暴露引用(在成员变量中定义好Thread对象)在run方法中直接 join前一个thread对象
- cyclicBarrier，一个CyclicBarrier实例控制两个线程，barrier在程序中执行await，一个线程在结束位置await，一个开始的位置await
- countdownLatch - 一个线程结束处countdown，另一个线程在开始处await，等待另一个线程countdown
- 使用semphore，一个线程在开始出acquire，尝试获得许可，另一个线程运行结束位置release，增加一个许可
- 多condition，一个线程在开始处condition.await 另一个线程结束处去signal唤醒
- 使用singleThreadExecutor，提交任务，顺序执行

## interrupt

使用

`Thread.interrupt`

可以为线程标记打断的标志

使用以下方法查询打断标志

`Thread.interrupted` - 查询状态结束后会清楚打断标记，重新标记为false

`Thread.isInterrupted` - 查询结束后不会清楚打断标记



对于sleep，wait，join这种会捕捉打断异常的，捕捉后都会清楚标记

## notiy(all)

notify死锁例子

线程1在synchronized(A)中使用nitofy()唤醒线程2，线程1接下来尝试synchronized(B)，但是线程2唤醒后立马抢到了B的锁并且在锁的逻辑中去synchronized(A) 使得线程1，2分别想要获得B，A造成死锁

notifyAll() - 不会从根本上杜绝死锁，而是让死锁发送的概率减小

使用notify()可能会造成死锁

wait配合while使用，防止虚假唤醒

如果WaitSet中的所有线程等待的是相同的条件，唤醒任意一个都能处理接下来的事物，可以notify

如果不是，一定要维护好 条件判断不满足，重新notify，自己回到WaitSet

## join原理

在一个线程内调用 A.join()

其实就是把线程A当作一个对象，调用了A的wait方法，此方法使用synchronized修饰

当A线程执行完毕，会由JVM调用notify方法





while (isAlive()) {

wait()}

## AQS使用双向队列原因

- 高效的节点取消 - 节点由于超时，中断，线程取消争抢锁，需要从链表中删除，单向链表需要遍历，双向链表根据原始节点，配合pre next 节点完成快速的节点删除
- 线程获取资源时候，需要检查前驱节点状态(检查是否已经释放锁)
- 在 `doReleaseShared`（共享模式释放资源）中，双向链表能高效传播唤醒信号，避免无效遍历。



next指针意义 - 快速定位后续有效节点



## volatile有序性 可见性实现原理

**volatile** 的底层实现原理是**内存屏障**，**Memory Barrier**（Memory Fence） 

- 对 volatile 变量的**写指令后**会加入**写屏障** 
- 对 volatile 变量的**读指令前**会加入**读屏障**

- 可见性 
  - 写屏障（sfence）保证在该屏障之前的 t1 对共享变量的改动，都同步到主存当中 
  - 而读屏障（lfence）保证在该屏障之后 t2 对共享变量的读取，加载的是主存中最新数据 
- 有序性 
  - **写屏障会确保指令重排序时，不会将写屏障之前的代码排在写屏障之后** 
  - **读屏障会确保指令重排序时，不会将读屏障之后的代码排在读屏障之前** 

**可见性**

对于volatile修饰的变量写操作直接写入主存，读操作直接从主存中获取最新值

通过CPU的**缓存一致性协议MESI**自动同步多核缓存 - 

当某线程修改 `volatile` 变量时，其他 CPU 核心的缓存行（Cache Line）会被标记为 **失效**，强制下次读取时从主内存重新加载。

**MESI协议详解**

volatile配合MESI协议实现共享变量的可见性

MESI将CPU缓存定义四种状态 - Modified - Exclusive - Shared - Invalid

**Modified** - 缓存行数据被修改而且没有跟新到主存，其他处理器的缓存行处于Invalid状态

**Exclusive** - 缓存行数据和主存保持一致，并且别的处理器没有缓存这个数据

**Shared** - 缓存行数据和主存保持一致，且被多个处理器所缓存

**Invalid** - 缓存行数据失效，需要从主内存重新加载



当一个处理器修改了缓存行，缓存数据状态变为Modified，通过总线通知其他处理器对应的缓存行变为Invalid

其他处理器读取缓存，需要重新从主存中去获取

volatile和MESI的配合

写操作 - 对于一个数据完成修改之后，强制让处理器将修改后的数据刷新到主存中去，并通过**总线**广播这个消息，其他处理器收到消息，将对应的缓存行标记为Invalid

读操作 - 对于一个数据的读取，首先判断缓存的状态，如果为Invalid，从主存中重新读取



MESI的局限性

- 缓存伪共享 - 多个不同无关变量存在于同意缓存行，导致缓存牵连共同失效
- 性能开销 - 频繁的广播总线和Cache Miss(缓存失效，从主存中重新获取) 消耗性能
- 仅仅通过MESI无法保证有序性，需要借助读写屏障实现volatile变量的有序性

**有序性**

JVM 会在不影响正确性的前提下，可以调整语句的执行顺序

这种特性称之为『指令重排』，**多线程下『指令重排』会影响正确性**

**原理**

指令可以再划分成一个个更小的阶段，例如，每条指令都可以分为： **`取指令 - 指令译码 - 执行指令 - 内存访问 - 数据写回`** 这 5 个阶段，在不改变程序结果的前提下，这些指令的各个阶段可以通过**重排序**和**组合**来实现指令级并行。现代 CPU 支持**多级指令流水线**，例如支持同时执行 `取指令 - 指令译码 - 执行指令 - 内存访问 - 数据写回` 的处理 器，就可以称之为**五级指令流水线**。**这时 CPU 可以在一个时钟周期内，同时运行五条指令的不同阶段**（相当于一 条执行时间最长的复杂指令），IPC = 1，本质上，流水线技术**并不能缩短单条指令的执行时间**，但它变相地**提高了 指令地吞吐率。**

**读写屏障解决有序性问题**

JMM在语言层面约束创造了Happens-Before规则，允许部分指令重排，但是要求最终结果正确

**如果操作 A Happens-Before 操作 B，则 A 的执行结果对 B 可见，且 A 的执行顺序先于 B**

满足happens-before规则的代码，一定可以保证有序性和可见性

例如

- 单线程中，代码顺序的执行结果与程序逻辑一致
- 对`volatile`变量的写操作 Happens-Before 后续对该变量的读操作。
- `synchronized`解锁操作 Happens-Before 后续的加锁操作。
- A Happens-Before B，且 B Happens-Before C，则 A Happens-Before C。
- `Thread.start()`方法的调用 Happens-Before 线程中任意操作。

## 指令重排序可能带来的问题

指令重排序在单线程下不会有问题(happens-before)，只有在多线程并发交错的环境下，可能出现一些问题

在多线程下由于重排序导致线程间的操作顺序混乱，比如数据的**可见性问题**(读到旧值)，**破坏逻辑因果关系**，在**代码具体逻辑下发生数据不一致问题**

以double-check为例

```java
public final class Singleton {
    private Singleton() { }
    private static volatile Singleton INSTANCE = null;
    public static Singleton getInstance() {
        // 实例没创建，才会进入内部的 synchronized代码块
        if (INSTANCE == null) { 
            synchronized (Singleton.class) { // t2
                // 也许有其它线程已经创建实例，所以再判断一次
                if (INSTANCE == null) { // t1
                    INSTANCE = new Singleton();
                }
            }
        }
        return INSTANCE;
    }
}
```

**其字节码执行指令中比较重要的几个步骤：**

- 17 表示创建对象，将对象引用入栈 // new Singleton 
- 20 表示复制一份对象引用 // 引用地址 
- 21 表示利用一个对象引用，调用构造方法 
- 24 表示利用一个对象引用，赋值给 static INSTANCE 



如果21 和 24发生调换

一个线程执行完成17，20，24此时对象饮用被赋值了，但是没有调用构造方法，也就是没有初始化，但是另一个线程此时进入逻辑，发现INSTANCE已经存在了，直接返回了，但是此时INSTANCE只是一个空壳，并没有初始化，造成了数据不一致问题

**如何避免指令重排序的问题？**

1. **使用`volatile`关键字**：通过内存屏障禁止重排序，并强制可见性。
2. **`synchronized`/`Lock`**：通过锁的释放 - 获取机制保证顺序性和可见性。
3. **显式内存屏障**：使用`Unsafe`类或 JVM 指令（如`StoreLoad`）插入屏障。
4. **遵循 Happens-Before 规则**：利用规则天然保证操作顺序（如`volatile`规则、锁规则）。

## AQS 共享模式

队列中的节点通过 `Node.SHARED` 标记为共享模式



1. 从队列头部开始，唤醒第一个共享模式的后继节点。
2. 若后继节点也是共享模式，继续唤醒其下一个节点，形成链式反应。
3. 通过双向指针 `next` 直接定位目标节点，无需遍历整个队列。
4. 若某节点被取消（`waitStatus == CANCELLED`），通过双向指针 `prev` 和 `next` 快速调整队列结构，跳过无效节点：



设置PROPAGATE状态

**`doReleaseShared` 方法**：当线程释放共享资源时，会调用该方法尝试唤醒队列中的后继节点。如果此时头节点的 `waitStatus` 为 0（表示没有设置任何唤醒标记），且队列中存在后继共享节点，会通过 `compareAndSetWaitStatus` 原子操作将头节点的状态设置为 `PROPAGATE`。这一操作触发了唤醒信号的传播机制，为后续节点的唤醒做好准备。示例代码如下：

- **`setHeadAndPropagate` 方法**：当一个共享节点成功获取到共享资源后，会调用该方法将自己设置为头节点。如果在设置头节点后，发现后继节点是共享模式，或者当前节点的 `waitStatus` 本身就是 `PROPAGATE`，就会继续尝试唤醒后继节点，并在必要时设置 `PROPAGATE` 状态，确保唤醒信号能够持续传播。

设置 `PROPAGATE` 状态的意义

- **保障唤醒信号传递**：高并发场景下，多个线程竞争和释放资源，`PROPAGATE` 状态确保唤醒信号不会因线程调度或资源不足而丢失，让所有等待的共享节点都有机会被唤醒。
- **提升并发性能**：减少了 `compareAndSetWaitStatus` 这类原子操作的使用，降低并发竞争。链式唤醒机制使多个共享节点连续被唤醒，减少线程在阻塞和唤醒状态间的切换，提高系统性能。
- **维护公平性**：在公平锁模式下，`PROPAGATE` 状态与 AQS 双向队列的 FIFO 特性结合，保证唤醒操作按队列顺序进行，先入队的线程先被唤醒。
- **适应复杂场景**：实际并发编程中，资源获取和释放情况复杂，`PROPAGATE` 状态让 AQS 能灵活应对各种边界条件，如多线程同时释放资源或新线程加入队列，确保系统稳定。

## 线程池常用阻塞队列

- ArrayBlockingQueue 数组实现fifo
- LinkedBlockingQueue 链表实现 fifo
- SynchronousQueue 不存储元素任务提交直接交给线程工作(核心线程无 - 创建临时线程) - 避免排队，减少延迟，快速触发拒绝任务
- PriorityBlockingQueue 优先级队列
- DelayQueue 延迟队列

## IO/CPU密集型线程池配置

网络请求密集型也就是IO密集型，因为它不是每时每刻都在用cpu

- 核心线程数 - IO n*2(充分利用CPU等待IO时间) CPU n + 1(避免线程切换开销)
- 最大线程数 - IO 大于等于核心(多一点不影响，并且可以应对突发流量) CPU 不要太多，不超过核心数(防止大量线程争抢cpu资源)
- 任务队列 - IO 无界队列，任务主要在IO耗时，任务多对CPU影响小 CPU - 有届队列防止大量任务积攒oom
- 拒绝策略 - IO 联动数据库(阻塞队列 + 拒绝策略) CPU - AbortPolicy，即使拒绝，防止CPU顶不住压力
- 最大存活时间 - IO 稍微长一点，网络请求线程服用  CPU - 短一点，及时回收资源，防止任务量过大



## ArrayBlockingQueue和LinkedBlockingQueue

- 一个使用数组，一个使用链表
- Array一定是设置有界限的，Linked可以是无界的，也可以有界
- Array生产者消费者采用同一把锁，Linked生产者消费者使用两把锁
- 内存连续，内存离散

## 读写锁原理

- ReentrantReadWriteLock读写锁，分为读锁和写锁
- 多个线程可以同时去获得读锁，但是不能有线程获得写锁
- 当有一个线程获得了写锁，会阻塞其他线程获得读锁和写锁



**state** int 变量 32位   **前16位**为**读锁**的获取次数   **后16位**为**写锁**的获取次数



**读锁获取**

**没有写锁被占用**（`state & 0xFFFF == 0`）-  state和一个4位的16进制做与预算，目标低16位都是0，所以结果是0

**读锁可以递增**（`(state >>> 16) + 1` 不会超限）位运算右移16位+1   < 65535(0xFFFF)

当前线程是持有写锁的线程（支持**写锁降级**）

**读锁释放**

`state = state - (1 << 16);` 



**写锁获取**

**写锁未被占用**（`state & 0xFFFF == 0`）。

**没有其他线程持有读锁**（`state >>> 16 == 0`）。

**可重入：当前线程已经持有写锁时可以再次获取。**

**写锁释放**

`state = state - 1;  // 写锁计数 -1`



持有写锁的可以降级成为读锁，但是读锁不能升级成为写锁



可以设置为公平锁

```Java
// 创建公平锁
ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock(true);
```



## AQS

早期程序员会**自己通过一种同步器去实现另一种相近的同步器**，例如用可重入锁去实现信号量，或反之。这显然不 够优雅，于是在 JSR166（java 规范提案）中创建了 **AQS**，提供了这种**通用的同步器机制。**

AQS 全称为 AbstractQueuedSychronizer ，抽象队列同步器。**阻塞式锁和相关的同步器工具**的框架 

ReentrantLock、CountDownLatch、Semaphore 都以AQS为基础。

共享独占

AQS首先使用一个volatile的state变量(**32bit int** 来维护同步状态，因为当时使用 **long 在很多平台下测试的结果并不理想** )代表锁的获取情况

维护一个类似于CLH的单向无锁队列(保证队列中线程的公平，不会线程饥饿)(FIFO保证入队线程的公平，自旋减少上下文切换) 将线程封装为节点Node

存储因为没有获取到锁的阻塞线程 他自身定义好了对队列的大部分操作

比如入队（while 配合cas 把tail替换成目标）出队（（while 尝试等待上一节点的状态变为释放锁 - 代表自己可以获取锁），删除等待超时或中断的线程节点，后续节点的唤醒(头节点释放资源后，自动寻找到第一个没有被取消的节点)，自旋检查前驱节点



具体的同步工具作为他的子类要重写tryAcquire和tryRelease 子类在其中去描述state的具体cas修改方法

在这里可以实现是否公平获取(获取锁之前是否判断entryList有对象) 是否支持可重入(记录目标线程，并每次重入做线程的判断) aqs比如阻塞获取为acquire 他其中会调用tryAcquire，并且自身完善好比如入队的操作 所以说子类重写好这两个基本功能 很多别的功能(获取锁超时机制)就被自动封装完成了



- **条件变量**来实现等待、唤醒机制，支持**多个条件变量**，类似于 Monitor 的 WaitSet



2. 阻塞恢复设计 
   - 早期的控制线程暂停和恢复的 api 有 suspend 和 resume，但它们是不可用的，因为**如果先调用的 resume  那么 suspend 将感知不到** 
   - 解决方法是使用 **park & unpark 来实现线程的暂停和恢复**，具体原理在之前讲过了，先 unpark 再 park 也没 问题 
   - park & unpark 是**针对线程**的，而不是针对同步器的，因此控制粒度更为精细 
   - park 线程还可以**通过 interrupt 打断** 



## synchronized 和 ReentrantLock

- 都可以重入
- 一个是关键字(修饰灵活)，一个是aqs的实现类
- synchronized代码块结束释放锁 控制不灵活
- ReentrantLock 锁打断 锁超时
- 公平性
- s使用简单 r复杂

synchronized - 简单场景 性能要求高

reentrantLock - 需要锁高级特性 复杂同步场景（中断超时公平 **Condition**）



低竞争 简单同步常景 synchronized性能好（优化 偏向锁 轻量锁）

高竞争 复杂同步场景 reentrantLock - cas 自旋 复杂场景不得不用reentrantLock



## ArrayList多线程问题

- 扩容操作包括创建一个新的数组并将原数组中的元素复制到新数组多线程同时执行可能导致数据丢失(丢失或覆盖)数组越界(数据写入错误位置)
- 数据丢失 添加失败
- 一个线程在扩容过程中完成了内存分配和数据复制，另一个线程又在此过程中进行扩容，可能会造成数组状态不一致或数据丢失。
- 读到脏数据 - 别的线程修改过程中的脏数据
- 索引错乱 多线程修改删除 + 读取

## LinkedList多线程问题

多线程的crud

- 节点丢失 一个线程修改节点的后继指针的时候 另外一个线程改变了其他节点的指针引用
- 破坏节点指针顺序 破坏链表结构 指针指向混乱
- 数据重复插入
- 不会有ConcurrentModificationException 但是导致后续节点遍历不符合预期



## CompletableFuture 对比 new Thread()

- cf 可以配合**线程池**使用 指定线程池加载任务
- cf 任务组合**链式调用**
- cf **异常**处理方便 链式编程中可以指定好异常处理的逻辑
- 支持直接获得**返回值** 或者阻塞等待返回结果

## 线程安全类

- vector
- CopyOnWriteArrayList CopyOnWriteArraySet
- ConcurrentHashMap ConcurrentSkipListMap
- BlockingQueue - ArrayBlockingQueue LinkedBlockingQueue
- Atomic（int array 字段跟新器）
- ReentrantLock
- Semaphore *3
- Collections.synchronizedMap,set
- ThreadLocal

## 超卖加锁保证原子性和一致性

- 单体架构 - synchronized \ reentrantLock
- 分布式锁 mysql zookeeper redis
- 数据库悲观锁 select .... for update
- 数据库乐观锁 版本号 时间戳
- 无锁 - Redis原子操作提前扣减库存(用户尝试在redis去扣减) 扣减成功就可以异步下单(压力转移到消息队列和redis)
- 无锁 - 海量并发 - 在redis无锁的基础上去将库存的记录分散到多个桶(也就是多个变量)这样用户就可以竞争多个桶

## 常用线程池

- Executors.newScheduledThreadPool 作为ScheduledExecutorService的实现类 - spring @Scheduled的底层
- newFixedThreadPool 只有核心线程所有fixed
- newCachedThreadPool 只有临时线程，数量取决于内存大小
- newSingleThreadExecutor 只有一个核心线程，实现任务顺序执行



## HashMap升级小总结

- 哈希扰动函数
- 数组链表 数组链表红黑树
- 头插法尾插法
- 扩容优化(2^n 快速取余) 高低位算法
- 先插入再扩容
- 1.8改进了对于`null`键值的支持，确保在多线程环境下不会出错。

## 线程安全的单例模式

- 直接饿汉 - 维护成类的静态变量
- 直接懒汉 配合synchronized 粒度大 效率低
- double check
- 维护成静态内部类的静态变量  不用锁 延迟加载
- 枚举单例 简洁，防止反序列化攻击，不用锁，不能懒加载

## 线程池之阻塞队列

**阻塞队列作用**

- 任务缓存排队
- 阻塞队列可以限制最大数量
- 线程协作，阻塞生产消费线程



**使用阻塞队列而非普通队列**

- 阻塞队列线程安全(ArrayBlockingQueue - 锁 concurrentBlockingQueue - cas)
- 同步机制，条件变量，阻塞和唤醒生产，消费线程
- 可以限制最大数量，普通队列一般只能限制初始大小，不能限制最大大小

## 任务到达线程池上线后的操作

**普通的拒绝策略**

- AbortPolicy
- DiscardPolicy
- CallerRunsPolicy
- DiscardOldestPolicy

**基本操作**就是增加最大线程数，或者说扩大阻塞队列的容量

**主流框架**

- Netty - 直接创建一个线程池之外的线程处理任务
- ActiveMQ - 在指定的时长内尽量让任务入队



任务持久化思路 - 自定义拒绝策略，当阻塞队列达到上限，临时线程耗尽，将任务持久化到Mysql，继承BlockingQueue实现混合的阻塞队列，修改取任务的逻辑，重写take()方法，取任务的时候优先从数据库中读取最早任务，数据库中没任务再从ArrayBlockingQueue中取任务，避免数据库中的任务饥饿



## 线程池线程异常，销毁复用情况

- **execute** - 异常会导致线程中止，并创建一个新线程替代
- **submit ** - 异常会封装在Future对象中，线程不会消失，继续复用

## 根据任务优先级执行的线程池

使用PriorityBlockingQueue优先级阻塞队列

线程安全，但是不支持阻塞操作

任务实现Comparable接口重写compareTo接口或者阻塞队列构造函数中指定Comparator对象(限定比较逻辑)

case：

- PriorityBlockingQueue本身是无界的，可能会OOM - 重写offer入队方法，插入元素超过阈值报错
- 低优先级任务可能饥饿，维护等待时间，重新入队跟新优先级？
- 其使用ReentrantLock实现同步，有性能消耗 - 可以接受



## 场景问题

现在要设计一个系统，有一个功能涉及到线程对锁的争抢，你认为在**不同的情况下**，也就是不同的系统背景下，一个**没有抢到锁的线程**应该去做什么

**考虑的点**

- 锁的目标是保护核心资源(金融余额)还是非核心操作(日志)
- 失败成本是否大 - 没有想到锁是不是会有用户体验影响
- 实时性，业务是否需要立即响应，是否允许延迟
- 资源限制 - 系统是否有足够的线程/cpu能力进行重试或者阻塞



**具体方案**

- 自旋挂起，在一个循环内，获取锁失败仅仅阻塞几毫秒然后直接重新尝试获取(业务要求一定要执行成功，系统资源充足，避免线程池资源耗尽) 自旋浪费性能，仅适用于锁竞争不激烈的场景
- 有限次数重试，仍失败则降级 - 大部分时间没有严重锁冲突(有限次数内就可以自旋获取到锁)极端情况允许获取锁失败(用户支持重新提交请求) 应该为每个线程设置随机的自选等待间隔(防止相互冲突，相互等待时间一样，集中重试)
- 阻塞挂起，等待被唤醒
- 直接返回失败 - 抢锁失败直接返回失败(锁竞争激烈，锁持有时间长重试没意义)(业务允许执行失败，可以后台或者用户重新操作)(高并发环境，系统保护自身资源，防止线程池资源耗尽) - 这大概率在并发量很大的场景下，所以可以配合限流操作，防止大量的失败请求使系统崩溃
- 执行降级逻辑 - 未抢到锁，执行次优方案 - (适用于失败成本低的场景，用户对返回要求不高) 比如显示实时数据的场景，没有抢到锁就返回一个缓存的旧数据
- 异步任务处理 - 将失败的请求加入MQ，后台处理 - 业务允许延迟，高峰时段锁竞争激烈的时期做调整



## 线程和进程

- 进程是程序的实例

- 线程是指令流，CPU顺序执行

- 进程是资源分配的最小单位，线程是资源调度的最小单位

- 进程之间相互独立，(一个进程崩溃不会影响其他进程，一个线程崩溃可能影响所在今臣)线程存在于进程之中，一个进程可以有多个线程

- 进程之间通信复杂，分为同一个计算机内的进程通信和不同计算机内的进程通信(网络)
- 进程创建销毁成本高
- 进程并发度低

- 线程通信简单，共享进程内存

- 线程轻量，上下文切换成本远低于进程

## 线程和进程调度

**同一个进程的多个线程可以到不同的cpu上吗**

可以

操作系统的调度器可以根据线程的优先级状态决定把线程分配到cpu的哪个核心

并且可以手动绑定线程到特定cpu执行，避免在不同核心之间频繁迁移



进程是资源分配到最小单位，线程是资源调度的最小单位



**线程调度**

- 负载均衡 - 避免有的繁忙，有的空闲
- 缓存 - 尽量让线程在之前执行过的cpu上运行，利用缓存数据
- 高优先级线程优先分配cpu资源



Java使用Thread.setPriority()设置线程优先级，映射到操作系统线程的优先级

操作系统根据优先级cpu因素分配线程



线程在cpu中迁移

- 负载均衡，动态调整
- 缓存失效，调度开销



**线程调度算法**(进程也差不多)

- 先来先到 - 按线程到达顺序执行

- 短作业优先 - 优先调用预计执行时间段的线程

- 时间片轮询，为每个线程轮询分配时间片，时间片用完挂起，并加入等待队列尾部，等待下一次分配时间片

- 优先级调度，根据线程优先级调度

- 多级反馈队列，创建多个队列，优先级越高时间片越小，优先调度高优先级队列，一个任务在分配的时间片内没有执行完成，降级到下一个队列

- 公平调度，确保每个用户获取cpu的时间片相同

- linux完全公平调度  -  维护线程的虚拟运行时间，并维护红黑树队列，查找虚拟运行时间最短的线程




线程特有的调度算法

- 抢占式 - 内核可以强制中断线程 分配cpu给更高优先级的线程
- 非抢占式 - 线程主动让出cpu
- 线程亲和性 - 将线程绑定到特定CPU核心，减少缓存失效和调度开发
- 负载均衡

**线程优先级**

```
Thread.getPriority();
Thread.setPriority();
```

优先级选择策略

- 不要盲目调高优先级，由于cpu调度的优化，盲目提高优先级可能得不到时间片
- IO密集型优先级应该降低，线程经常处于阻塞状态，cpu占有率低
- CPU密集型，不要设置过高优先级，可能会导致线程饥饿
- 对延迟敏感的线程提高优先级，但也要注意饥饿

## **优先级低的线程一直得不到执行怎么办**

**问题根源**

- 优先级差异过大
- 调度算法有缺陷
- 高优先级线程发送死锁长时间占用cpu



**解决**

- 避免使用最高级别的优先权
- 定期让高级别的线程查看自己的运行时间，周期性让出时间片
- 动态调整优先级
- 改用公平的线程调度算法 - 时间片轮询，多级反馈队列
- 比如使用公平锁
- 修改程序逻辑，减少高级别线程任务执行量，把耗时部分移除锁代码块
- 使用tryLock - 超时后增加优先级
- 分离架构，把低优先级的任务抽离出来，单独在一个独立进程中运行



## 进程通信方式

- 管道 - 单向数据流 - 基于内核缓冲区 - 匿名管道仅用于父子进程 - 命名管道支持无关队列
- 消息队列 - 内核管理，通过唯一标识符读写消息支持异步通信
- 共享内存 - 多个进程映射同一块物理内存
- 套接字 - 夸网络通信，支持TCP UDP
- 信号量 - 计数器，控制多进程对共享资源的PV操作，实现同步

## 进程同步方式

- 信号量 - 整数计数器，P等待 ， V通知 控制资源访问， 二进制信号量 - 值0/1 计数信号量 - 限制资源数量
- 互斥锁 - 同一时间只有一个进程获取🔒
- 条件变量 - 配合互斥锁使用 - 条件不满足阻塞后 后被唤醒
- 读写锁
- 屏障 - 多个进程运行到屏障点统一放行

同步目的

- 保证资源不被同时访问
- 协调顺序