# Redis

## Redis底层数据结构

**SDS** - Simple Dynamic String 44

**二进制安全**(不使用/0作为字符串的结束标志)

在字符串的对象头中记录**字符串的长度**，获取长度的时候不用每次重新计算(O1)

redis对其做了**内存预分配**(linux分配内存涉及用户态和内核态的转换，应用程序无法直接访问内核，消耗性能)

扩容会有一个新字符串大小(动态扩容)

这个大小如果小于1M，新空间为 扩容后大小 * 2 + 1

如果大于1M，为扩容后大小+1M + 1(避免过多冗余)



**IntSet** - 基于整数数组，**有序**，长度可变

有2 4 8字节的整数**编码格式**，每个元素编码方式一样，占用内存一样，方便指针直接寻址

扩容的时候(编码方式字节升级) 使用**倒序拷贝**，防止元素覆盖

底层使用**二分查找来保证顺序**



**Dict** -字典 类hash表

一个Dict主要包含两个hash表，和一个rehash的进度标识

hash表 - 主要包含一个entry数组，拉链发解决冲突，哈希表大小，主要用于取余(& num-1) 还有entry 个数



每次新增键值对，会检查负载因子(used / total)

当负载因子大于1，并且没有进行bgsave或者bgrewriteaof之类的后台操作，就扩容，如果大于5了就不管了，直接扩容

扩容的大小为第一个大于等于used + 1的2的n次

如果小于0.1就会做收缩

第一个大于等于used的2的n次



扩容操作，首先创建一个新的hash结构，将rehash标识设置为0

这里使用渐进式hash，不会在扩容的时候直接把所有数据完成迁移，每次执行增删改查，去查看rehash标识位，如果是，将操作的对象同步到新的hash表，新增操作直接写入新hash，直到所有数据完成迁移，复位标志位，交换链表



**ZipList**

特殊的双向链表

首先其不使用指针定位前后元素，因为内存是连续的，在每个节点中去维护前一节点的长度，通过这个，定位到前序节点的位置

这个长度的保存和长度值有关

如果前一节点长度小于254字节，使用一个字节保存

如果大于，使用5个字节保存，其中第一个字节为标志位

连锁跟新问题 - 假设前一节点数据发生变化，长度超过了254，后续节点将长度为变为5个字节后，自身的长度也超过了254，。。。循环导致后续的内存连锁跟新

数据过多，查询性能差



**QuickList**

使用指针实现的链表

节点元素为ZipList

解决ZipList连续内存空间不足的问题

并且有特殊算法压缩中间节点，进一步节省内存



**SkipList**

元素有一个score值，根据这个值做升序排序，建立多级指针(悉数索引)

层级越高跨度越大



**RedisObject**

每个数据结构被统一封装成RedisObject，里面保存一些数据结构的元信息，比如对象类型，编码方式，最后一次的访问时间，引用计数器，最后是真实对象地址指针



## Redis数据结构

**String**

如果字符串长度小于44字节，此时对象头和其是一段连续空间，只需要调用一次内存分配函数，效率高，不会产生内存碎片

如果存储字符串值为整形，大小在LONG的范围内，使用int编码，不需要SDS



**List**

低版本 - 元素数量小，内存小ZipList 不然使用LinkedList **512/64**

高版本- 统一使用QuickList



**Set**

当set所有元素都是整数，并且元素数量不超过设定值，使用intset

不然使用HT，DICT编码，元素值为null **512/整数**



**ZSet**

使用skipList完成有score的元素的排序

使用dict完成元素和score的对应 **128/64**



元素数量少，并且元素内存不大的时候，使用zipList完成

由于其本身没有复杂的元素存储规则或者排序规则，将两个挨在一起的元素视为一个整体，元素在前，score在后，按照score升序排序



**Hash**

数据量小

使用ZipList - 相邻entry = field + value

数据量大 使用dict编码 **512/64**

## Redis - AOF

**先执行写操作再把操作追加到aop文件**

- 保证记录进入文件的都是没有语法问题的语句，不用在做aof重做的时候再一次检查语法
- 不会影响阻塞当前命令的执行
- 操作执行和记录不是一个原子事件，可能存在数据丢失风险
- 可能影响阻塞下一个命令的执行(硬盘io压力大，写入速度慢，阻塞后续操作)



**aof文件写入时机**

redis执行完写操作将命令加入aof缓冲区 (调用write) 将缓冲区内容加入aof文件(page cache) (fsync命令) 内核缓冲区内容落盘

三种写回策略控制的就是fsync执行的时机

- Always - 每次执行完都写入磁盘
- EverySec - 每次写入内核缓冲区，每秒执行一次fsync落盘
- No，每次写入内核缓冲区，由操作系统决定何时写回硬盘



**aof重写**

由于aof记录了所有写命令，写命令前后执行对象可能会相同，造成前指令没有存在的意义，导致aof文件过大

redis判断aof文件超过一定阈值，开启重写，读取数据库中的所有键值对，对每个键值对用一个命令去描述记录到新的aof文件，全部记录完成做一个替换

不会复用原来的aof 文件，而是创建一个新的做替换，防止重写过程中发生意外，导致aof文件污染



**aof后台重写**

重写过程复杂，速度慢，需要后台子进程来重写bgrewriteaof

**进程vs线程**

如果使用子线程，多线程之间共享内存，在修改共享内存的时候需要加锁，降低性能

对于子进程，和父进程共享内存数据，但是是read only，当任意一方发生写的操作，会发生写时复制copy-on-write，各自拥有独立的副本

页表是虚拟内存和物理内存的映射关系，fork的时候是拷贝一份页表，而不是直接拷贝数据，两个进程访问各自的页表来访问相同的数据

**两个阻塞时机**

- fork创建子进程的时候，需要拷贝父进程的页表
- 有一方发生写操作的时候，触发CopyOnWrite，需要拷贝真实内存(对于修改数据的内存做一份复制，极端情况下所有都会复制一遍，导致内存占用变成原来2倍)



在执行后台aof重写的时候，如果有新的写命令，会把命令加入到aof重写缓冲区，当子进程完成重写工作，会把aof重写缓冲区中的内容追加到重写的结果中去，然后替换掉旧的aof文件(当然这个命令也会加入到旧的aof文件，防止重写失败，aof文件发送数据不一致)

## Redis - RDB

save和bgsave

服务器启动自动加载rdb文件

可以配置每隔一段时间进行一次快照生成(在多少时间内完成多少次修改)



bgsave的时候，会fork子进程

子进程进行快照记录的时候，如果主进程发生了写操作，会触发copy-on-write，但是此时的写操作无法被记录，也就是无法被加入到此次的快照执行中去，一定要到下一次的bgsave快照

如果系统在bgsave刚完成崩溃，快照生成时的那些数据无法被保存

## RDB AOF混合

在aof重写的时候工作

fork出来的子进程会把和主进程共享的数据以RDB的形势写入到AOF文件，主线程的操作缓冲在aof重写缓冲区，最后把这些操作追加到aof文件，最后做aof文件的替换

文件的前半段时rdb，后半段时追加的aof命令

综合了两个优点：**恢复速度快，数据完整性高**

## Redis BigKey对持久化影响

- AOF使用always策略(redis操作的执行和aof文件的写入不是原子操作，为了最大限度保证数据一致性，使用always策略)，每次写入key，都要进行write + fsync的落盘操作，大key会导致主线程阻塞时间长
- fork出子进程的时候，如果大key特别多，导致页表很大，复制耗时，fork在主线程中执行，会阻塞主线程
- 发生copy-on-write的时候，要拷贝数据真实的数据，导致效率低

## Redis fork耗时太大

info命令获取latest_fork_usec指标，标识redis最近一次fork操作耗时

**可能原因**

- 内存占用高，页表的复制开销大
- 系统负载高，服务器CPU使用率高，fork进程调度缓慢
- 内存碎片，频繁的键值对增删改导致内存碎片化，需要遍历更多的内存页

**优化**

- 控制实例的内存在10G以下，执行速度会很快 - 数据结构优化，合理配置内存淘汰策略
- 如果redis作为纯缓存，没有数据安全问题，不建议开启aof和rdb，不会有fork
- 主从架构中，增大repl-backlog-buffer内存，减少全量同步执行的概率，尽量每次都是增量同步
- 将Redis和高cpu进程隔离部署或者在业务高峰期禁止bgsave或者aof重写
- 使用redis分片集群
- 定期重启redis，重新加载数据，重建内存布局



## Redis过期删除策略

Redis会把所有设置了过期时间的key加入到一个过期字典中去管理，key是一个指针，指向键，value保存过期时间

通过比对过期时间和系统时间，判断是否过期	



redis不使用定时删除(为每个key维护一个计时器，虽然精准，内存压力小，但是性能开销大)

使用惰性删除+定期删除

在访问一个key之前，首先判断是否过期

每隔一个周期，从过期字典中抽取20个key，删除过期key，如果删除比例超过5个，就再抽取20个，不然就结束本次抽取

## Redis内存淘汰策略

- 不进行内存淘汰，但是拒绝写入(默认)
- LRU LFU *2
- 随机淘汰设置了过期时间的键
- 淘汰过期时间最少的键
- 在所有键中随机淘汰





**LRU**

redis不使用常规的LRU算法 - 维护一个链表，操作完一个节点就跟新到头节点，删除最旧的就是删除尾节点

**缺点**

- 链表存储需要额外的存储空间
- 大量数据访问需要不断维护链表顺序涉及大量的元素位置变更，影响性能

Redis使用近似的LRU策略 - 在RedisObject中维护一个最后一次访问时间的字段，每次淘汰的时候，随机挑选5个key，淘汰最久没有使用的那个

- 不用维护链表
- 不用做频繁的数据移动
- 无法解决**缓存污染** - 有些数据被读取并缓存，但是只使用了那一次，但是会在缓存中存活很长一段时间



**LFU**

在LRU模式下，RedisObject的lru字段记录上次的访问时间，在LFU模式下，一样的字段有不同的记录含义

高16位存储上一次访问时间，低16位存储逻辑访问频次

频次不是简单的访问次数累加，而是根据时间得出的类似访问概率的标识

每次访问一个key，首先做频次的衰减，会根据本次访问的时间和记录的上一次访问时间的差值进行判断，差值越大，衰减越多，代表访问概率越小，越容易被淘汰

然后做一个概率的增加，概率越大的key，增加的概率和幅度越小

提供两个参数可以去控制增长和衰减的速度



## Redis主从复制

**全量同步**

steps

- 首先从服务器执行replicaof命令，并发送一个数据同步请求(请求携带主服务器runID和offset)
- 主服务器接收到数据同步请求，发现runID和offset都为空，判断出需要进行全量同步
- 主服务器执行bgsave生成RDB文件发送给从服务器，从服务器清空当前数据并加载RDB文件
- 主服务器将生成穿出RDB文件期间以及从服务器加载RDB文件期间产生的数据记录在replication buffer中
- 当从服务器RDB文件加载完成，主服务器将replication buffer中的数据传输给从服务器，加载
- 主从服务器完成第一次同步后，双方维护一个TCP连接(长连接)
- 后续主服务器有写操作命令会通过这个连接传输给从服务器以保持一致性



**增量同步**

steps

- 主从之间网络断开并恢复后，从服务器重新发送一个数据同步请求(runID + offset)
- 主服务器接收到请求，发现runID存在并且offset不是-1，代表尝试进行增量同步
- 主服务器每次执行写操作后，不仅会通过TCP传输给子服务器，还会记录到repl_backlog_buffer中，并用replication offset标记位置
- 此时主服务器通过请求中的offset判断从服务器需要同步的数据是否repl_backlog_buffer中是否可以满足(环形，可能被覆盖)(增量同步/全量同步)
- 如果是全量同步，执行bgsave
- 如果是增量同步，将请求的offset和目前的replication offset的差数据写入replication buffer，并传输给从服务器



为了避免频繁的全量同步，应该**适当增大**repl_backlog_buffer的内存，防止数据容易循环覆盖





**主从节点存活检测**

节点之间使用ping-pong心态检测机制，一半以上的节点去ping一个节点没有收到pong的响应，认为其死亡，断开连接

- 主节点每隔10sping从节点，判断存活性
- 从节点每隔1s向主节点上报offset，一方面同步数据，另一方面检测主从之间的网络状态



**主从架构中key的过期处理**

主节点删除了一个key或者使用淘汰算法淘汰了一个key，会模拟一条del命令发送给从节点并执行



Redis主节点每次收到写命令，现实把命令写入replication buffer 缓冲区再统一异步发送给从节点



**replication buffer and repl_backlog_buffer**

- 前者出现在全量和增量，后者只出现在增量
- 前者一个主节点为每个从节点都分配一个，后者一个主节点只有一个
- 前者满了，会断开连接，重新连接，重新全量同步
- 后者满了是环形结构，会从头重新覆盖



**主从数据不一致**

- 主从节点存在数据不一致 - 主从命令复制异步执行，主节点不会等待从节点执行完成才返回执行结果，导致从节点可能还没执行命令
- 保证主从之间网络通信良好，处于同一个机房
- 编写一个外部监控程序，获取主节点的replication offset和从节点的offset，监控其差值，当差值大于一定阈值，标记从节点不可用，让客户端直接从主节点读取数据

- 网络抖动(主从维护TCP长连接) - 为主从建立专用网络通道，监测网络延迟，设置阈值警告
- repl-backlog-buffer太小了，每次要全量同步，速度慢，有延迟概率 - 增大repl-backlog-buffer大小
- 短时间大量写入操作超过主从节点的同步能力 - 流量控制+大事物拆分(追求一致性的路上肯定要有所牺牲)
- 主从切换导致数据不一致 - 对主节点开启强制数据确认机制，有多少从节点数据同步成功才算成功

**主从切换数据丢失**

- 异步复制同步丢失

由于主节点和从节点的数据传输是异步的，当主节点接受了一个写请求还没来得及传输给从节点发生了宕机，会导致数据丢失

**solve-**

配置max-slaves-max-lag 一旦所有的主从节点之间的延迟都超过这个阈值，主节点就拒绝写入操作，可以保证把主从的数据差控制在这个阈值内

但为了系统的高可用性，我们在发生这一情况的时候应该进行降级策略，客户端发现redis无法写入的时候，将数据写入kafka队列，当redis恢复正常，消费Kafka数据

- 集群脑裂

主节点的网络发生了暂时性的问题，与从节点断开连接，其实本身没有宕机，客户端不知道主节点的问题，照常写入数据，但是主节点无法同步给从节点。此时哨兵发现主节点挂了，重新选举了一个主节点。此时原来的主节点恢复了，并且被哨兵降级，他会向新的主节点做一次全量同步，导致数据丢失

**solve-**

配置两个参数

min-slaves-to-write- x 主节点至少要有x个从节点连接

min-slaves-max-lag-x  主从延迟不能超过x秒

当上述两个配置不成立，主节点禁止写入，报错给客户端

这样子的话，当主节点被标记为死亡，哨兵重新选举一个leader后，原来的主节点没有暂存数据，恢复后变成从节点不会有数据丢失，新的数据直接写入到新的主节点中去了

## Redis哨兵

**监控 选主 通知**



**判断节点故障**

哨兵会以集群的方式部署，一个哨兵通过ping-pong方式每隔一s发现某个节点故障代表主观下限

回向其他哨兵发起命令，其他哨兵收到命令会去判断连接，作出赞成或者反对，如果赞成数量超过阈值(一般设置为总数一半)

代表客观下线



**转移哨兵选举**

决定由哪个哨兵来进行主从故障转移

作出客观下线的那个哨兵就是候选者，候选者回向其他哨兵发出命令，表明希望成为leader执行主从切换，并让其他哨兵投票

每个哨兵有一次投票机会，只有候选者可以投给自己

当一个哨兵获得的票超过半数并且大于配置文件中的阈值就竞选成功了

每个候选者会先投自己一票，然后向其他哨兵发出请求，收到请求的哨兵如果还没有投过票，就会把票投给他



哨兵集群至少要有3个

如果是两个，投票阈值也要是2，如果是1，选举者自己选自己就结束了但如果是2，一旦有一个哨兵宕机，就无法完成切换工作了

可以判断主观下限条件 - 哨兵投票数量大于阈值quorum

可以完成主从切换(竞选可以成功) - 哨兵投票数量大于哨兵数量一半，并且大于阈值quorum

**选举哨兵的意义**

- 确保只有一个哨兵执行任务
- 如果不经过选举，多个发现问题的哨兵可能在同一时间都进行故障转移的操作，导致数据并发问题(脑裂风险)
- 随机选择需要一个全局协调者，比如zooKeeper 违背Sentinel去中心化的思想
- 需要借助分布式锁，比如使用redis的RedLock还会涉及哨兵，问题循环了
- 让哨兵节点形成共识

**主从故障转移**

1 - 首先选出主节点

- 排除网络状态不好的从节点
- 选取优先级更高的从节点
- 选取offset越大的从节点，代表数据和主节点差异越少
- 最后选取运行ID越小的从节点

2 - 哨兵让选中的从节点执行slaveof no one，不成为任何人的奴隶！成为主节点(哨兵会以1s间隔的频率对其发送命令，观察其执行状态，直到完成升级)

3 - 让所有其他节点执行slave of命令

4 - 通知客户端主节点更换(客户端和哨兵建立连接，客户端会订阅哨兵提供的频道，当主节点发生变化，哨兵回向频道发送新节点ip信息等，客户端收到信息跟新数据)

5 - 当旧主节点重新上线，哨兵也会让他执行slave of命令，成为从节点



哨兵之间通过Redis的发布者订阅机制相互发现- 主节点上有一个哨兵的频道，每个哨兵启动将自己的信息上传到频道，其他哨兵订阅了这个频道，可以获得到新哨兵的信息并建立连接



哨兵建立的时候，只需要填写主节点的信息 - 哨兵回向主节点发送请求，请求所有的从节点信息，通过主节点上获取的信息而和所有从节点建立连接

## Redis Cluster

主从有两个问题没有解决 - 海量数据无法存储，没有解决高并发写的问题



**客户端对分片的访问 - 哈希槽**

redis cluster很大一个特定就是，请求随意发一个节点可以自动转发到正确位置

key - CRC16算法 - 散列 取余16384



客户端任意请求一个节点，客户端根据key计算槽的位置，判断是否属于自己，是自己就返回数据，否则重定向

- Moved重定向 - 这个槽不是本节点的，返回这个key应该存在的实例的ip和port，客户端根据这个重定向发送key
- ASK重定向 - 发生于集群伸缩的时候，集群伸缩导致槽迁移，此时的重定向为ask



**节点通信**

使用Gossip协议，一个节点想要和其他节点分享数据，周期性随机选择一些节点，并把信息传输给这些节点，收到信息的节点会做同样的事情，知道所有节点都收到信息

redis中节点通过集群总线 - cluster bus 进行通信，使用特殊的端口号，为其对外端口号+10000





**故障转移**

- 首先节点之间通过ping-pong机制彼此感知存活状态，一个节点对另一个节点ping没反应，标记主观下限，半数以上节点都主观下线，标记为客观下线，彼此通过gossip交流信息
- 从故障的主节点的从节点中选取一个offset最大，运行时间最长，节点id最小的从节点作为候选人
- 候选人给主节点发送投票请求，半数通过则成功
- 候选人执行slave no one 发布消息广播自己成功 将槽位映射过来 客户端重定向



**cluster哈希槽位之16384**

CRC16 算法产生的哈希值是16位的 理论可以有2^16 65535个槽

原因是，redis每个实例节点保存他对应有哪些槽，彼此交流数据，检测心跳的时候，这个槽信息是会相互传递的

如果槽有65535 过多，槽元信息的存储内存变大，节点数量多了，总体占用过多



如果进一步使用8192再小一点的槽数量也是不合理的

- 一方面此时数量级小了，内存减少的效果变小了
- 另一方面hash冲突变大了



## Redis大key

网络阻塞 数据倾斜 阻塞单线程 cpu压力(序列化反序列化) fork rewrite速度变慢 内存占用大 主从延迟

- 合适数据结构

- 压缩文本数据

- 使用redis分片集群

- 拆分key或均匀散裂插槽(key:1 , key:2 ... 程序随机访问一个)

- 修改内存淘汰策略

- 定期清理

- CDN节点(在全球范围内部署多个缓存节点，我们将大key，热点key添加进入cdn缓存，用户请求资源的时候，会根据地理位置，将用户请求路由到最近的CDN节点，降低延迟，提高响应速度)

## Redis分布式应用

- 缓存
- 临时存储(时效性用户标识)
- 分布式锁
- 消息队列
- 计数排行榜
- 漏桶限流(将当前计数与允许的速率限制进行比较，如果计数在速率限制范围内，则处理请求。如果计数超过速率限制，则拒绝请求)
- 数值统计 bitMap或者HyperLogLog



## Redis快的原因

- 纯内存操作存储
- 单线程模型，避免线程上下文切换(避免线程安全问题，cpu不是制约性能的瓶劲，内存和网络带宽才是)
- 高效丰富的数据结构
- 非阻塞IO多路复用技术
- c语言实现，效率高

## Redission的锁实现

- **Reentrant Lock** - 可冲入锁，通过hash存储锁信息，键为名字，值为重入计数
- **Fair Lock** - 公平锁，在可冲入锁的基础上维护一个等待队列，新获取锁的请求线程会加入等待队列
- **MultiLock** - 联锁，将多个独立的锁合并为一个逻辑锁(将不同锁实现作为连锁的构造参数，尝试对连锁加锁会顺序尝试对每个子锁加锁，任意一个子锁获取失败，释放所有已经加锁的子锁)
- **Red Lock** - 红锁，针对分布式Redis集群的方案，尝试加锁，只有超过半数的节点加锁成功，才能认为加锁成功(将不同锁的实现作为红锁的构造参数，对红锁整体进行加锁，超过半数加锁成功，标记加锁整体成功)
- **ReadWriteLock** - 读锁共享，写锁独占，底层仍是两个hash结构记录线程和计数
- **Semaphore** - 信号量，控制同时访问资源的最大线程数量，通过redis的AtomicLong存储剩余许可数，获取和释放使用Lua脚本保证原子性
- **PermitExpirableSemaphore** - 在信号量的基础上增加许可的过期时间，通过hash结构存储许可的过期时间

## Redis和Memcached区别

- Redis数据结构更丰富，memcached只支持key-value
- Redis支持持久化数据，memcached纯内存操作
- Redis支持原生的集群，memcached不支持原生集群
- Redis支持更多的高级功能，发布订阅模型，lua脚本，事物，memcached不支持

# Mysql

## 数据库三大范式

- 每一列是不可分割的原子数据项(一个列选项中不要选择多个信息的柔和)
- 每一列和主键完全相关，不能和主键的一部分相关(一个表中同时有商品和用户信息，主键应该为商品id和用户id的复合，这就坏了)

- 列要和主键直接相关而不是间接相关(列要和主键是直接对应关系，主键不能作为中间跳板成为两个列的连接，比如某一列可以通过主键搜索到另一列信息从而直接相关，这是不可以的)

## in exist优化

子查询变成左右连接

- 避免重复查询子表（每一行主查询都会执行一次子查询）join 将2️⃣表合并 减少io
- 优化策略可以更好的执行索引 避免子查询中的全表查询
- 减少子查询临时表开销 - 在内存中存储一个临时表 join在内存中执行连接操作
- 减少锁竞争
- join有更多的 执行计划 引擎优化空间大 嵌套 哈希 排序等 子查询固定执行计划 无法复杂优化



## **索引优化可以考虑的点**

统筹规划 - explain 查看执行策略

- 对查询 范围 连接join 排序分组字段建立索引







- 多个单列索引优化成联合索引 防止mysql仅选择一个单列索引别的索引不使用
- 调整联合索引顺序 满足字段使用的最左前缀法则
- 索引覆盖 避免select *
- 为迎合where 的字段 完善联合索引 [create(a,b,c) select a,b,c from table where a = ? and b = ?]
- 区分度高的字段建立索引 放在联合索引第一列







- 避免索引失效
- 使用高版本的mysql 使用索引下推
- 选择合适索引类型(b hash text 空间索引R-tree)
- 避免对null上使用索引





## 慢查询具体排查 

*一条sql可能出现的问题* - 

**语法层面**的拼写，关键字错误  

**性能层面**的全表扫描，索引，嵌套查询，表连接问题

**数据层面**的数据类型匹配(过长的字符串插入到长度有限的字符列中)，空值处理，数据不一致问题

**安全层面**的sql注入问题(用户输入作为拼接而不是填充)

**逻辑层面**sql本身没有写正确，执行结果无法达到预期

*对于sql的优化分为多方面*-

- 维护数据一致性 - 事物
- 安全 - 防止sql注入
- 最重要的还是性能方面的优化，提升执行速度还有降低内存cpu消耗



考虑的*point*

- 数据量，并发量
- 索引错误使用
- sql语句本身书写逻辑
- 表结构
- 业务逻辑结构设计
- 数据库实例性能





1. MySQL 的最大连接数设置是否适应你当前的应用负载 - max_connections最大连接数 阻塞新连接
2. **多人多版本开发问题**， 后人增加索引， 但是优化器还是走了老索引， **force**强制走索引
3. 查看监控Buffer Pool的命中情况， 一般低于99%， 一般考虑是业务逻辑有问题， 违背了局部性原理(程序在某一段时间内，往往会重复访问一部分数据)， 抖动太多(搜索查询范围广，查询结果不稳定，导致缓存页buffer pool 频繁变动) 或者说突发原因导致了buffer pool的大小很小
4. 索引失效： explain 看 走的索引、类型、长度、 extra、 搜到rows 判断， 并修改
5. 也有可能是redo log太小，脏页刷新到磁盘的时机不行(被迫不断刷新)



**具体操作**

- explain 查看执行计划(索引 排序)

**索引优化**

- 选择合适索引类型(b hash text 空间索引R-tree)

- 限制每张表的索引个数
- 根据需求判断 创建比如联合索引或者前缀索引
- 联合索引避免索引失效，区分度最高，使用最频繁的数据列在最左侧
- 多个单列索引优化成联合索引 防止mysql仅选择一个单列索引别的索引不使用
- 覆盖索引
- 避免索引失效
- where 排序建立索引
- 确保排序字段的升降和索引建立的升降关系一致

**sql语句本身优化**

- 避免使用select *
- join多表联查，小表驱动大表(把小表加入内存，对于小表每个数据在大表中进行一次匹配，总查询次数为小表行数，看似总查询次数都是m * n 但是对于小表来说，在大表中的匹配可以使用索引，效率高，随意决定效率的就是左侧表的总行数)
- 用union合并数据的时候，如果允许出现重复数据，使用union all union会自动去重，耗时
- 子查询变为左右连接
- 少范围的范围查询变成in 可以用索引(in的目标为确定集合，可以优化成and的等值查询)
- 对于分页查询，使用where ID > ? limit 结构
- 批量操作，防止每一条单独语句的执行进行一次网络请求的建立

**表结构优化**

表结构的设计是避免慢sql的开始

- 经常使用的列放在同一个表，避免多表联查
- 打破范式设计，在表中保留一些冗余字段(比如在一个电商表中增加一列厂商地址，本来存储厂商表id做联查，现在可以直接获取，在高并发环境下提高效率，虽然维护一致性优点麻烦)，避免多表联查
- 选择合适数据类型，内存占用越小越好
- 避免在普通表中使用text类似的大数据，避免直接存储文件本身， 可以上传云空间存储地址，如果一定要用，将大字段独立到单独表中去
- 不要使用字符串存日期，占用空间大，存储效率低，无法使用日期相关api
- 单表字段做冷热分离，分成两张表

**架构优化**

- 读写分离
- 分库分表
- 缓存
- 硬件优化

## mysql分库分表机制

**分库方式**

- shardingJDBC 基于AOP 对SQL进行拦截，解析，改写，路由 自行编码，java语言
- MyCat 中间键，不调整代码实现分库分表，无感切换





**分库分表原因**

- 单表数据量大，索引满，慢查询
- 并发压力大 qps爆满
- 业务复杂度高，多模块耦合在一个库中
- 实现高可用 地理部署







- 根据时间节点水平分
- 根据地理位置分库，部署在不同地区的服务器
- 按照用户id 取模
- 一个系统有多个模块，为每个模块分出来一个库
- 用某个字段去做hash







- 垂直拆分后，多表多库查询，数据一致性问题，同时查询成功
- 管理分布式事务
- 若数据分布不均匀，产生性能瓶颈，数据倾斜
- 数据同步一致性问题(本来在同一个表 同一个库里面 现在要操组多个)
- 运维的监控工作
- 后续的数据迁移成本
- 可能导致外键等完整性约束失效，索引失效，查询变慢，需要重新抉择设计

## 一条select sql执行的全过程

- 建立连接，验证身份权限
- 查询缓存，命中直接返回
- 解析sql，分析构成词法，语法
- 执行 - 预处理 -检查select字段是否存在，将*换为具体的列名
- 执行 - 优化 - 抉择出效率最高的执行计划(存在多个索引的情况下)
- 执行 - 根据计划执行，读取记录返回



## 一条update sql执行全过程

- (通过索引)找到这个数据，直接从Buffer Pool返回？从磁盘中加载到Buffer Pool？
- 判断update是否有意义(update内容是否真的会让数据发生变更)如是则将操作穿给引擎层
- 引擎层开启事物，update操作前记录undo log
- 更新内存，记录脏页，WAL记录redo log(顺序写 效率高)
- 记录bin log
- binlog 两阶段提交

## 主从同步总结

Replication - 最常见的主从模型 主库读写，从库读

读写分离，故障恢复，水平扩展(降低单机io)

**流程**

- IO thread - 从库创建IO thread 连接主节点，请求跟新binlog 接受binlog后写入relay log
- dump thread - 主库收到从库的binlog请求，创建一个dump thread把binlog 同步给从库
- sql thread - 从库读取relay log 解析并执行

**主从延迟问题**

- 从库机器性能差，CPO 磁盘IO性能差，命令执行慢，占用时间长，创建IO线程和sql线程吃力，创建出来也分不到时间片(**提高性能**)
- 从库查询压力大，负载均衡设置不合理，大量读请求在某一个从库，cpu资源被查询线程占用，影响sql thread的执行(负载均衡)
- 主库大事物执行多，事物结束才写入binlog(降低食物复杂度，大事物变成小事物)
- 低版本mysql，sql thread只支持单线程(使用高版本)
- 主从表配置不一致 对于一个修改语句(binlog 命令模式) 主库直接使用索引，从库只能全表查询(优化索引)
- 时效性高的select操作强制从主裤执行
- 优化binlog 模式 三个模式(sql thread 执行)
- semi-sync配合异步复制(保证主从的数据一致性) 半同步等待超时后自动降级为异步复制

**数据丢失问题**

**三种主从复制策略**

- 异步复制 - 主库执行事物直接返回结果，不关心从库是否完成 - 效率高但是数据容易丢失(从库log同步之前主库宕机)
- 全同步复制 - 主库提交事物并且等待从库完成数据同步后向客户端返回结果 - 性能差，数据安全
- 半同步复制(semi-sync) 主库提交并等待至少一个从库接受binlog并且写入relay log

## 主从之双主架构

两个节点互为主从，每个节点都可以进行读写，并且接受并执行另外主节点的binlog

**优势**

- 高可用，故障转移
- 读写性能
- 地域冗余(根据地区部署主节点)
- 业务友好(将不同业务分配到不同主节点)

**问题**

- **复制冲突**(两个节点同时修改同一条数据有冲突)(业务约束，不同业务写入不同节点)(比较语句时间，保留新的)
- **循环复制节点**A同步给B一条数据，B生成这个数据的binlog记录又同步给A(记录命令的创建id - server id，两次过后A再执行发现id和自己一样不执行)
- **主从延迟变大**每个写操作要经历两次同步(A-B B-A)
- 只能保证最终一致性，不能实时一致性

## 主从之主备复制

双主节点中，同一时刻只有一个主节点提供写服务

- 两个主节点配置一样，故障转移块
- 不用考虑循环复制问题



可以为每个主节点再加一个从节点用来提供读服务

## 主从之环形复制

多个主库之间形成环形，前一个节点是当前节点的主库，当前节点是前一个节点的从库也是后一个节点的主库

一个节点出问题就会有循环复制问题(server id 为问题节点ID，除了他不能停止)

## mysql分页

LIMIT offset，limit

先跳过offet个消息记录，取limit

如果offset非常大， 导致大量磁盘io，**速度慢**

**数据一致性问题**

分页内容排序消耗大量cpu和内存资源



记录上一次分页的最后id 下一次 where ID > Id limit 10 速度变快

加锁

创建索引并且增大缓冲区



我们使用分页的时候，肯定会去基于某个字段的排序

如果这个字段是主键，就会使用聚集索引，把offset + limit的数据行都加载到server层，然后舍弃offset数据

如果是普通字段，类似主键操作，就是先提取主键，然后做一次回表，再把数据加载到server，无法避免数据无效加载



where id > ？这个方法固然有效，但是在order by之后我们怎么获取 这个id呢

我们本来可能select * from limit x y

先做一步select id limit x 1，不同于加载整个数据项，只加载主键到server层，然后提取一个数据，效率可以有一步提升

但是全局来看，offset非常大的分页操作，就是存在性能问题 - 深度分页问题



无法解决，只能优化

比如对于分页 页数的上线不要太离谱，分10w页根本顶不住的

如果把直接输入分页的页码，换成上一页，下一页性能会好很多，因为每次可以记录此次分页数据的主键id

或者使用瀑布流的方式，上下滚动，记录id

## Mysql 并发承载量

- 高性能cpu
- 固态硬盘
- 提高Buffer Pool的容量 减少数据从磁盘加载到缓冲池的时间
- 增加 max_connections
- 早期版本 增加查询缓冲池的大小
- 启动慢查询日志
- sql优化
- 分页优化
- 索引优化
- 主从 分库分表
- 使用缓存

## sql优化

**基础层面**

- 索引优化(高频率字段加索引，最左，覆盖索引，索引下推)
- 查询语句优化(select *  exist-in  子查询-join  where id > ? limit (分页优化)) **where having判断不重合**
- where 判断中 尽量不使用函数，将函数转换成比较，防止索引失效

**架构层面**

- 将表中的大字段拆分到副表
- 分表(时间，地区)
- 抉择合适的**数据类型**
- 明确事物，避免事物太大，导致主从延迟
- 少用悲观锁 优先使用乐观锁
- 隔离级别RR -> RC
- 主从集群
- 二级Redis缓存

**实战优化**

- explain 关注索引和extra
- 调整Buffer pool
- 批量提交
- 开启慢查询日志

## distinct和group by对比

distinct 和 group by都可以聚合某一个字段达到去重的目的

**用法**

distinct 后的字段全部参与

**SELECT DISTINCT col1, col2  -- 对 col1 和 col2 的组合去重 FROM table;** select后不能部分distinct部分不distinct

groupby 配合聚合函数(sum max)使用



**结论**

有索引并且语意等价的情况性能接近，无索引distinct更快



group by做的工作是先分组再聚合

一般使用聚合函数作为select的目标，但是如果不使用聚合函数且不是分组列(很多个数据行分组项一样但是 其他列不同，select了其他列)

不同的索引引擎会选取不同的结果，mysql8.0之前会做隐式排序然后返回某一个(临时表 + filesort)

有索引情况两者都是根据索引去做快速的判断，性能接近

groupby 可以配合聚合函数完成复杂的功能，十分灵活

假如distinct一个基本不重复的字段比如时间戳，反而效率低

要根据**索引 数据量，版本综合考虑**

## MySQL中不使用NULL作为默认值的原因

- Null需要额外的存储位来记录该字段是否为null，多null会有额外的存储消耗
- 查询条件包含null，导致索引失效
- null影响groupBy(出现不符合预期的分组，null会成为一个独立分组) 或者 distinct操作
- 程序中select的时候，对于字段需要做非null的判断才能赋值
- 对于存在null的表，mysql优化器会拒绝多字段的索引合并优化，选择全表扫描(index1 or index2)

## 分页sql一般会遇到哪些问题

- 性能问题 - offset n 要扫描n条数据后才去做 limit 而不是直接跳过(数据库中需要保留查询中所有被跳过的记录) - 使用索引+where id > n
- 数据不一致问题 - 使用普通的offset + limit，第一次查询完后，插入一条数据，导致数据行排序整体后移一行，下一次分页可能查询到老数据 - 使用固定排序条件(比如使用主键的排序) - 使用where id > ? limit 的策略也可以实现
- 分页查询的排序问题 - 对某个字段进行排序后分页查询，每次对于相同字段的排序可能每次不一样，可能导致两次分页的结果集中有重复内容 - 确保字段唯一或者增加一个id的排序(主字段，副字段)
- 边界问题 - 防止查询页数超过数据总量 - 提供给前端count(*)的接口，初步做数据存在性的判断 - 后端使用正确的offset计算方法(size * (page - 1)) 防止最后一页找不到
- null问题 - 不同数据库对于null的排序规则不一致，可能放在最前面，可能最后面 - 排序的where中去指定is not null (排序操作中，排序字段为null)

## Mysql数据存储

创建一个database之后会在mysql目录下创建一个以database名字命名的目录保存数据(三个文件)

- 数据库默认字符集，字符校验规则
- 表结构信息
- .ibd文件的表数据信息



表空间 - segment - extent - page - row

**row** - 一行数据为row

**page ** - 数据库从磁盘读取数据以页为单位 - 16KB

**extent** - 页之间通过链表维护，相邻的页被保存在同一空间，组织维护在区中，避免大量随机IO

**segment** - 表空间分为多个段，每个段有自己独立完整的空间结构 - 索引段(非叶子节点区)，数据段(叶子节点区)



行格式分为4种 **Redundant**(老，被抛弃) **Compact**(紧凑) **Dynamic** **Compressed**(based on compact)



Compact 模式 - 额外信息 + 行隐藏列信息 + 真实数据

![截屏2025-04-02 11.01.16](https://typora---------image.oss-cn-beijing.aliyuncs.com/%E6%88%AA%E5%B1%8F2025-04-02%2011.01.16.png)

如果行中字段存在变长字符varchar，则会存在一个**变长字段长度列表**，里面会存储每个变长字符实际使用的字符长度(存储顺序和列顺序相反) n<256 1个字节存储，不然2个

记录头信息中存储下一个记录的指针，具体指向的位置是记录头信息和真实数据(隐藏数据)之间的位置，向左读的第一个变长记录，和向右读的第一个变长字符串可以对应，并且最主要的原因是记录和字符串的距离近，处于同一缓存行的概率大，提高CPU Cache的命中率



null值不会直接存储在真实数据中，会记录在**NULL值列表**中，以二进制表示null，1代表字段为null，0代表字段不为null，同样是逆序存放，并且null值列表以字节为单位(8bit，不足自动补全)

如果每个字段都规定为not null ，那么就不会存在这个null值列表，至少节省1个字节空间



**记录头信息**

- 此条数据是否被删除的标志
- 下一条数据地址 - 记录头信息和真实数据(隐藏数据)之间的位置
- 当前记录类型 - 普通记录，非叶子节点记录。。。。。。



row_id - 隐藏主键 6字节

trx_id - 操作本记录的事物id 6字节

roll_pointer - 记录上一版本的指针 7字节





一行数据最大内存为65535字节(普通对象，不包括比如text的大对象)，varchar（n） n的最大值，应该为65535减去变长字符列表，null值列表，记录头信息，隐藏字段？？？ 如果是utf-8编码模式，一个字符最多3个字节，具体指为(65535-n) / 3



行溢出 - 一页数据16KB，最多只有16384字节数据，但是数据最大65535字节，或者比如text大对象，内存占用更大，存储方式为，在真实数据区域存储一部分数据，并维护存储一个20字节的溢出页地址，在溢出页中存储完整信息(compact) Dynamic Compressed采取的措施是完全行溢出，原始行中不存储真实数据，仅仅记录20字节的溢出页地址，全部数据存储在溢出页中



## count汇总

count(*) = count(1) > count(主键) > count(字段)

count(字段) - 统计字段不为null的记录数量

count(1) - 总计表总数量



**count(主键)** - 

在server层维护count变量，如果有二级索引就用二级索引，不然使用聚集索引遍历记录，主键不为null就计数+1

因为二级索引中也有主键值，二级索引数据量小，遍历效率高

遍历的时候，把读取的每一个记录一次传到server层，判断主键是否为null +1

**count(1) -** 

server层维护count变量，同样的索引选择流程遍历记录，但是不会判断具体某个值，记录存在就往server层抛一个1，不用返回具体数据行，相比count主键效率高

**count(*) -** 

mysql官方文档中写count*使用与count(1)相同的方式去实现，效率一样

count(*) -> 直接转换为count(0)

**count(字段) -** 

全表扫描，最慢



由于InnoDB支持事物，同一时刻的多版本并发查询要保证数据的隔离性，所以不能直接维护一个变量如同MyISAM记录总行数



优化 - 

explain select count(*) form table 执行结果中会有rows的估计值

或者手动创建新表或者什么redis记录行数

## 幻读场景

**case1**

事物A使用快照读(普通select) 读取一个范围，不加锁

事物B在上述范围增加一条数据比如id = 5

事物A去跟新id = 5的数据(不要说为怎么会跟新本来不存在的数据行...)

此时id = 5的记录行的trx_id已经变成了事物Aid，再去快照度就出现了

**case2**

事物A使用快照读(普通select) 读取一个范围，不加锁

事物B在上述范围增加一条数据比如id = 5

事物A使用当前读去读取上述范围，直接出现幻读



这种幻读往往发生在比较特殊的场景，可以在事物一开始就进行一次当前读，加上 next-key lock(RR隔离级别往上，对当前读的间隙锁定)

## RepeatableRead中的不可重复读？

快照读 - 根据MVCC读取数据版本

当前读 - 加锁读，读取数据最新版本



现有以下流程 ：

事物A开启，事物A快照读，事物B开启并修改数据，事物A当前读

会发现两次读取数据不一致，看似出现了不可重复读

因为事物B修改数据并提交之后，事物A当前读，读取到了最新数据



但是不可重复读的定义是 - 一条相同的select语句，前后两次执行结果不一致

这里最多说是 当前读和快照读不一致问题



前后如果同时当前读(前加锁，后阻塞)，快照读(MVCC)都不会发生不可重复读

## mysql死锁

gap lock - 行锁，存在SX冲突

gap lock 没有冲突

next-key lock 存在SX冲突



我们要插入一条数据 首先判断数据是否存在，插入id 1007 1008，首先分别判断1007 和 1008都是否存在(两个事物)

判断1007，select for update，还有1008 两个事物插入之前都被对方的间隙锁阻塞了，等待对方释放锁，间隙锁可以共存

插入的事物想在这个区间内产生一个插入意向🔒(特殊的间隙锁，锁一个位置)两个事物不能一个有某个区间的间隙锁，一个有这个区间上的插入意向🔒

**better case**

主键排序后16 20

我要插入17 18 首先做select for update，由于对索引加锁(数据项不是索引，全表遍历，对每个数据行加临键锁，表现出来像是表锁)，gap lock范围都是16-20

插入的时候，会有插入意向锁，特殊的gap lock

两个事物，不能在同一时间，一个拥有间隙锁，另一个拥有该间隙的插入意向锁

事物A尝试插入的时候，发现了B的gap lock，无法插入，生成插入意向锁 (表明插入的意向)与意向锁不冲突，和gap lock冲突(多个事物尝试插入同一个间隙的时候，假如插入意向锁，相互之间不会阻塞，冲突，防止死锁，只有发现这个间隙有别的锁 - gap lock才会阻塞)



还有就是普通的思索，RR隔离级别下，锁在事物提交后才会释放，所以两个事物分别访问两条数据，然后交叉再访问一次，就会进入死锁状态



解决- 

设置事物等待锁超时时间，超时后直接回滚



开启mysql死锁检测，监测到死锁后，会自动回滚一个事物

## 关于mysql日志

undo log - MVCC 原子性

redo log - 持久性，故障恢复数据不丢失 顺序读



对undo log的修改也会记录到redo log (每秒刷盘，提交事物刷盘，保证持久性)



**redo log**

产生一条redo log，不是直接持久化到磁盘，会先将redo log加入到redo log buffer，后续统一刷新到磁盘

刷盘时机：

Mysql正常关闭

redo log buffer 记录到写入量大于redo log buffer 内存空间一半

InnoDB后台线程每隔1s自动刷新(redo log 在事物提交之前就可能持久化到磁盘)

每次事物提交

其中每次事物提交的刷盘策略可以细分，由一个参数控制，012分别对应

- 将redo log 内容留在buffer中，不主动写入磁盘(mysql进程崩溃，会导致上一秒事物数据丢失)
- 事物提交，将缓存在buffer中的内容直接持久化到磁盘
- 将缓存内容写入到磁盘page cache页缓存，并不意味着直接写入磁盘(操作系统崩溃，上一秒数据丢失)



后台线程 - redo log  - write() -  page cache  - fsync() - 磁盘



redo log出现的目的是为了标记脏页，当脏页刷新到磁盘后，redo log对应的记录就没有必要了

InnoDB中存在redo log group 重做日志组，两个文件循环交替写

redo log日志本身也是循环写用wrtie pos标记当前写入的位置，用checkpoint标记要清理的位置

当write pos要追上checkpoint的时候，代表记满了，新记录要覆盖还没有执行的就log 了

此时会阻塞系统，刷盘，所以redo log要适当大一点



**redo 和 bin**

- 一个server层，一个引擎层
- 格式不同，bin三种，redo为物理日志，记录什么数据做了什么修改
- bin追加写入，全量数据，redo 循环写入
- bin用于备份恢复，主从复制，redo 用于异常恢复



**bin log**

事物执行时，把日记写到binlog cache 事物提交将binlog cache 写到binlog 文件

每个线程有自己的binlog cache，最终写到一个binlog文件

有参数控制刷盘策略 0 1 N

- 每次事物提交，将cache write加入到操作系统页缓存，后续由操作系统fsync刷盘
- 每次事物提交write + fsync直接落盘

- 每次事物提交write，N次后再fsync  




事物提交的时候，binlog 和 redo log会同时提交，所以会有两阶段提交

发生宕机，恢复的时候，去寻找状态为prepare的redo log记录，根据他的内部事务id去binlog中寻找，如果可以找到，代表binlog落盘成功，只是没有来得及回调成功接口，否则回滚事物



每次提交事物都要落盘两次，IO开销大

锁竞争激烈(背)



组提交概念-  多个事物同时提交，将多个binlog刷盘操作合并成一个，减少磁盘io次数



mysql 磁盘io高

- 设置组提交
- binlog 落盘使用N策略，先把cache写入操作系统页缓存，N此后统一fasync
- redo log 设置为每次提交，将数据写入页缓存，而非直接写入磁盘

## Buffer Pool

Buffer Pool默认大小128MB

缓存数据页 一页16KB 

![截屏2025-04-07 19.03.24](https://typora---------image.oss-cn-beijing.aliyuncs.com/%E6%88%AA%E5%B1%8F2025-04-07%2019.03.24.png)

每个缓存页会对应一个控制块，缓存快和缓存页无法将整个Buffer Pool填充满，所以会产生碎片空间

控制块中会记录一系列元信息，包括缓存块地址，链表节点等



使用**空闲链表**来管理空闲页，将空闲的缓存页对应的控制块作为双向链表的节点组织起来

当从磁盘中加载一个页到Buffer Pool的时候，从Free 链表中取出一个空闲页，填充对应的缓存页，然后把这个节点从链表中remove，因为是双向链表，可以快速移除

![截屏2025-04-07 19.24.53](https://typora---------image.oss-cn-beijing.aliyuncs.com/%E6%88%AA%E5%B1%8F2025-04-07%2019.24.53.png)

类似的，设计Flush 链表，用于管理脏页，后台线程遍历Flush链表，写入磁盘



**buffer pool的淘汰策略**



普通的LRU算法 - 页存在，跟新到链表头部，页不存在，删除尾页，加载页到头部

**预读失效问题** - 程序满足局部性原则(数据的访问是局部的，靠近当前访问数据的数据，大概率在未来也会被访问到) 为了降低磁盘io，加载某一数据页的时候，会把它临近的都加载进来

但是如果临近页在头部一直没有被访问，反而尾部可能的热点数据页被淘汰了，增加了io成本，得不偿失



我们把整个LRU区域分为两份，一个young区，一个old区 63:37

加载一个页后，加载的页放到young的头部，预读页加载到old区的头部

因为old比较小，所以预读的页被淘汰的概率大

使得访问概率大的数据页，尽量处于稳定(young区)不会被预读页所干扰



**Buffer Pool 污染问题**

Buffer Pool的数据页被全部替换，导致热点数据流失，需要重新io操作

产生原因：

- select扫描了大量范围
- 错误使用索引，导致全表扫描
- 程序不满足局部性原则，访问数据变化大，程序逻辑有问题

本质原因是，old区因为被访问一次而进入young区，条件太宽松

mysql 为进入young区的条件增加一个停留在old区时间的判断。第一次访问old节点，在控制块记录访问时间，下一次访问，对比时间间隔，小于设定值就认定升级无效



为了防止young的频繁变动，young区域千1/4不会参与头的变动，只有剩余3/4参与



**脏页刷新 -** 

- redo log日志满了
- Buffer Pool空间不足，淘汰一部分数据页，如果数据页是脏页，就刷盘
- mysql空闲时，后台线程定期刷新脏页到磁盘
- mysql 正常关闭前刷新脏页到磁盘